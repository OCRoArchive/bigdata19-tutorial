{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Parallel Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Parallel Training\n",
    "\n",
    "- we can have multiple GPUs on a single machine\n",
    "- how can we take advantage of them?\n",
    "- data parallel training:\n",
    "    - split input batch into multiple smaller batches\n",
    "    - train the same model on each GPU\n",
    "    - sum up the gradients across GPUs\n",
    "    - update the weights consistently across GPUs\n",
    "- implemented by `torch.nn.DataParallel`\n",
    "- simple modification of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Sun Dec  8 02:56:34 UTC 2019\n",
      "tmbcomp\n",
      "tmb\n",
      "/home/tmb/exp/bigdata19\n",
      "158.153.83.34.bc.googleusercontent.com\n",
      "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-ffa2b7fc-dde2-eb1c-7481-861ea0f181a3)\n",
      "GPU 1: Tesla V100-SXM2-16GB (UUID: GPU-80d5ad59-f785-94da-ec20-cb43f2114bd1)\n",
      "GPU 2: Tesla V100-SXM2-16GB (UUID: GPU-e1896053-8373-94a6-c0d3-febdfe9312ca)\n",
      "GPU 3: Tesla V100-SXM2-16GB (UUID: GPU-ba5fd29a-104c-ac84-b13d-f04ff2f6f9bc)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torchvision/io/_video_opt.py:17: UserWarning: video reader based on ffmpeg c++ ops not available\n",
      "  warnings.warn(\"video reader based on ffmpeg c++ ops not available\")\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "!date; hostname; whoami; pwd; curl https://ipinfo.io/hostname; nvidia-smi -L\n",
    "from imp import reload\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchmore import layers, flex\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import imagenet\n",
    "import os.path\n",
    "from torch.utils import data as torchdata\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mock Loader for Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class MockLoader(object):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        self.data = torch.rand(shape).cuda()\n",
    "        self.targets = torch.zeros((shape[0],), dtype=torch.int64)\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield self.data, self.targets\n",
    "            \n",
    "batch_size = 128\n",
    "training_dl = MockLoader((batch_size, 3, 224, 224))\n",
    "inputs, targets = next(iter(training_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Serial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "def make_model():\n",
    "    return models.resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "trainer = helpers.Trainer(model.cuda())\n",
    "trainer.set_lr(1e-6)\n",
    "trainer.train_for(5000, training_dl, quiet=True)\n",
    "serial = trainer.timers.training/batch_size\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataParallel Model\n",
    "\n",
    "Note: only one change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "model = nn.DataParallel(make_model())\n",
    "trainer = helpers.Trainer(model.cuda())\n",
    "trainer.set_lr(1e-6)\n",
    "trainer.train_for(5000, training_dl, quiet=True)\n",
    "parallel = trainer.timers.training/batch_size\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003332657925784588 0.003342693066224456 0.9969978875592062\n"
     ]
    }
   ],
   "source": [
    "print(serial, parallel, serial/parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bigger Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "training_dl = MockLoader((batch_size, 3, 224, 224))\n",
    "\n",
    "model = make_model()\n",
    "model = nn.DataParallel(make_model())\n",
    "trainer = helpers.Trainer(model.cuda())\n",
    "trainer.set_lr(1e-6)\n",
    "trainer.train_for(5000, training_dl, quiet=True)\n",
    "parallel512 = trainer.timers.training/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003332657925784588 0.003342693066224456 0.0012320765294134618 2.704911461442352\n"
     ]
    }
   ],
   "source": [
    "print(serial, parallel, parallel512, serial/parallel512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Combining DataParallel with FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "0.0012320765294134618 0.0009898143354803324 1.2447551881692696\n"
     ]
    }
   ],
   "source": [
    "from apex import amp\n",
    "\n",
    "class ParallelAmpTrainer(helpers.Trainer):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(model)\n",
    "    def set_lr(self, lr):\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)\n",
    "        self.model, self.optimizer = amp.initialize(\n",
    "            self.model, optimizer, opt_level=\"O1\", loss_scale=\"dynamic\")\n",
    "        self.model = nn.DataParallel(self.model)\n",
    "        \n",
    "batch_size = 512\n",
    "training_dl = MockLoader((batch_size, 3, 224, 224))\n",
    "\n",
    "model = make_model()\n",
    "trainer = ParallelAmpTrainer(model.cuda())\n",
    "trainer.set_lr(1e-6)\n",
    "trainer.train_for(5000, training_dl, quiet=True)\n",
    "parallelamp = trainer.timers.training/batch_size\n",
    "print(parallel512, parallelamp, parallel512/parallelamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataParallel\n",
    "\n",
    "- `DataParallel` is a simple way of using multiple GPUs\n",
    "- effectively you end up with much bigger batch sizes\n",
    "- may create an I/O bottleneck\n",
    "- consider using `DistributedDataParallel` even on a single node"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
